{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "\n",
    "#忽略警告\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "### 加载数据\n",
    "- 将每行数据分成两部分：英语&中文\n",
    "- 在分割完成的英文和中文数据，开头加上\"BOS\",结尾加上\"EOS\"\n",
    "- 英文数据使用nltk进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    en=[]\n",
    "    cn=[]\n",
    "    num_examples=0\n",
    "    with open(in_file,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip().split(\"\\t\")\n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            # split chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "train_file = \"data/train.txt\"\n",
    "dev_file = \"data/dev.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建单词表\n",
    "`collections.Counter`用法\n",
    "- counter工具用于支持便捷和快速的计数，例如：\n",
    "``` python\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:\n",
    "    cnt[word] += 1\n",
    "print cnt\n",
    "输出:Counter({'blue': 3, 'red': 2, 'green': 1})\n",
    "```\n",
    "- `counter.most_common(n)`从多到少返回一个长度为n的列表\n",
    "``` python\n",
    "输入：Counter('abracadabra').most_common(3)\n",
    "输出：[('a', 5), ('r', 2), ('b', 2)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX=0\n",
    "PAD_IDX=1\n",
    "def build_dict(sentences,max_words=50000):\n",
    "    word_count=Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s]+=1\n",
    "    ls=word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 2\n",
    "    word2id = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    word2id[\"UNK\"] = UNK_IDX\n",
    "    word2id[\"PAD\"] = PAD_IDX\n",
    "    return word2id, total_words\n",
    "\n",
    "en_word2id, en_total_words = build_dict(train_en)\n",
    "cn_word2id, cn_total_words = build_dict(train_cn)\n",
    "en_id2word = {v: k for k, v in en_word2id.items()}\n",
    "cn_id2word = {v: k for k, v in cn_word2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将单词转变为数字\n",
    "- `dict.get(key,default=None)`\n",
    "    - key -- 字典中要查找的键\n",
    "    - default -- 如果指定键的值不存在时，返回该默认值\n",
    "    ``` python \n",
    "    dict = {'Name': 'Runoob', 'Age': 27}\n",
    "    print \"Value : %s\" %  dict.get('Age')\n",
    "    print \"Value : %s\" %  dict.get('Sex', \"Never\")\n",
    "    ```\n",
    "    - 输出：\n",
    "        * Value : 27\n",
    "        - Value : Never"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_word2id, cn_word2id, sort_by_len=True):\n",
    "\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_word2id.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_word2id.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_word2id, cn_word2id)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_word2id, cn_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS 我 有 很 多 才 能 。 EOS\n",
      "BOS i have many abilities . EOS\n"
     ]
    }
   ],
   "source": [
    "#查看数据集\n",
    "k = 1000\n",
    "print(\" \".join([cn_id2word[i] for i in train_cn[k]]))\n",
    "print(\" \".join([en_id2word[i] for i in train_en[k]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据集划分为batch\n",
    "- **`get_minibatches(n, batch_size, shuffle=True)`**\n",
    "    - n：数据集的大小\n",
    "    - batch_size：batch的大小\n",
    "    - shuffle：是否对原数据进行打乱\n",
    "    - 返回(batches)：列表(其中包含若干个长度为batch_size大小的数组)\n",
    "- **`prepare_data(seqs)`**\n",
    "    - seqs：一个batch的数据\n",
    "    - 该函数是将一个batch的数据转换成矩阵的形式\n",
    "    - 该矩阵的维度：高度为该batch中数据的个数；宽度为该batch中最长的数据的长度\n",
    "    - 返回：x(该矩阵)，x_lengths(该batch中每条数据的长度)\n",
    "- **`gen_examples(en_sentences, cn_sentences, batch_size)`**\n",
    "    - en_sentences:英文数据\n",
    "    - cn_sentences:中文数据\n",
    "    - batch_size:每个batch的大小\n",
    "    - 返回(all_ex):列表,其中包含若干个元组，每个元组有四个元素:\n",
    "        - 英文数据矩阵\n",
    "         ；英文数据长度\n",
    "        ；中文数据矩阵\n",
    "        ；中文数据长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(n, batch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, batch_size) # [0, 1, ..., n-1]\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    batches = []\n",
    "    for idx in idx_list:\n",
    "        batches.append(np.arange(idx, min(idx + batch_size, n)))\n",
    "    return batches\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths #x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq(无Attention机制)\n",
    "### 定义模型(无Attention机制)\n",
    "> ####  PlainEncoder()\n",
    ">     \n",
    "- **`torch.sort(input, dim=-1, descending=False, out=None) -> (Tensor, LongTensor)`**\n",
    "    - [Pytorch官方文档](https://pytorch.org/docs/stable/torch.html#torch.sort)\n",
    "    - 返回：排序后的数据；排序后的数据原本所处的位置\n",
    "- **`nn.utils.rnn.pack_padded_sequence & pad_packed_sequence`**\n",
    "    - [Pytorch官方文档](https://pytorch.org/docs/stable/nn.html#pack-padded-sequence)\n",
    "    - 主要是为了解决RNN的输入数据，不等长的问题\n",
    "- **`torch.Tensor.contiguous()`**\n",
    "    - [Pytorch官方文档](https://pytorch.org/docs/stable/tensors.html?highlight=contiguous#torch.Tensor.contiguous)\n",
    "    - 主要是整合数据，让数据在内存上连续存在之类的\n",
    "- 返回hid[[-1]]是为了取最后一层的最后一个cell的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        #这里要排序的原因，是因为对于pack_padded_sequence要求输入是排过序的\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        \n",
    "        #此处相当于对一个tensor调了下顺序，然后在物理内存上再利用contiguous()将数据放在一起\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        return out, hid[[-1]]\n",
    "\n",
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "#         print(output_seq.shape)\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        output = F.log_softmax(self.out(output_seq), -1)\n",
    "        \n",
    "        return output, hid\n",
    "    \n",
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid = self.decoder(y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, None\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=10):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            \n",
    "        return torch.cat(preds, 1), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        # input: (batch_size * seq_len) * vocab_size\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        # target: batch_size * 1\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encoder = PlainEncoder(vocab_size=en_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = PlainSeq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义验证、训练以及测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            # 更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "\n",
    "                \n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.087850570678711\n",
      "Epoch 0 iteration 100 loss 5.22370719909668\n",
      "Epoch 0 iteration 200 loss 4.53439998626709\n",
      "Epoch 0 Training loss 5.4803422334586624\n",
      "Evaluation loss 4.825616509067588\n",
      "Epoch 1 iteration 0 loss 4.375795364379883\n",
      "Epoch 1 iteration 100 loss 4.662783622741699\n",
      "Epoch 1 iteration 200 loss 3.956834554672241\n",
      "Epoch 1 Training loss 4.588299141118519\n",
      "Epoch 2 iteration 0 loss 3.8004751205444336\n",
      "Epoch 2 iteration 100 loss 4.271549224853516\n",
      "Epoch 2 iteration 200 loss 3.6715335845947266\n",
      "Epoch 2 Training loss 4.195793269916067\n",
      "Epoch 3 iteration 0 loss 3.491119146347046\n",
      "Epoch 3 iteration 100 loss 3.97506046295166\n",
      "Epoch 3 iteration 200 loss 3.478689432144165\n",
      "Epoch 3 Training loss 3.9394883778967533\n",
      "Epoch 4 iteration 0 loss 3.286635637283325\n",
      "Epoch 4 iteration 100 loss 3.7937796115875244\n",
      "Epoch 4 iteration 200 loss 3.294625759124756\n",
      "Epoch 4 Training loss 3.749932619476352\n",
      "Epoch 5 iteration 0 loss 3.109724283218384\n",
      "Epoch 5 iteration 100 loss 3.6471171379089355\n",
      "Epoch 5 iteration 200 loss 3.172715663909912\n",
      "Epoch 5 Training loss 3.604048237205675\n",
      "Evaluation loss 3.6867348706564678\n",
      "Epoch 6 iteration 0 loss 3.005718231201172\n",
      "Epoch 6 iteration 100 loss 3.5088469982147217\n",
      "Epoch 6 iteration 200 loss 3.032308340072632\n",
      "Epoch 6 Training loss 3.481593692223928\n",
      "Epoch 7 iteration 0 loss 2.846039295196533\n",
      "Epoch 7 iteration 100 loss 3.4045419692993164\n",
      "Epoch 7 iteration 200 loss 2.9434494972229004\n",
      "Epoch 7 Training loss 3.378191752915598\n",
      "Epoch 8 iteration 0 loss 2.7644119262695312\n",
      "Epoch 8 iteration 100 loss 3.325059175491333\n",
      "Epoch 8 iteration 200 loss 2.868847608566284\n",
      "Epoch 8 Training loss 3.286872225471614\n",
      "Epoch 9 iteration 0 loss 2.654723644256592\n",
      "Epoch 9 iteration 100 loss 3.2557196617126465\n",
      "Epoch 9 iteration 200 loss 2.786505699157715\n",
      "Epoch 9 Training loss 3.205929148198372\n",
      "Epoch 10 iteration 0 loss 2.5730767250061035\n",
      "Epoch 10 iteration 100 loss 3.1635518074035645\n",
      "Epoch 10 iteration 200 loss 2.720855236053467\n",
      "Epoch 10 Training loss 3.1347648733635487\n",
      "Evaluation loss 3.423294957768453\n",
      "Epoch 11 iteration 0 loss 2.4859135150909424\n",
      "Epoch 11 iteration 100 loss 3.0998899936676025\n",
      "Epoch 11 iteration 200 loss 2.660094976425171\n",
      "Epoch 11 Training loss 3.067913526802669\n",
      "Epoch 12 iteration 0 loss 2.4303951263427734\n",
      "Epoch 12 iteration 100 loss 3.0079705715179443\n",
      "Epoch 12 iteration 200 loss 2.5385429859161377\n",
      "Epoch 12 Training loss 3.0084769909590157\n",
      "Epoch 13 iteration 0 loss 2.369328498840332\n",
      "Epoch 13 iteration 100 loss 2.993586778640747\n",
      "Epoch 13 iteration 200 loss 2.5323965549468994\n",
      "Epoch 13 Training loss 2.9532898536590855\n",
      "Epoch 14 iteration 0 loss 2.319735527038574\n",
      "Epoch 14 iteration 100 loss 2.946546792984009\n",
      "Epoch 14 iteration 200 loss 2.486826181411743\n",
      "Epoch 14 Training loss 2.903144798529454\n",
      "Epoch 15 iteration 0 loss 2.240452766418457\n",
      "Epoch 15 iteration 100 loss 2.8862502574920654\n",
      "Epoch 15 iteration 200 loss 2.462130546569824\n",
      "Epoch 15 Training loss 2.854838613682157\n",
      "Evaluation loss 3.3169508923044955\n",
      "Epoch 16 iteration 0 loss 2.1576921939849854\n",
      "Epoch 16 iteration 100 loss 2.8367669582366943\n",
      "Epoch 16 iteration 200 loss 2.3741583824157715\n",
      "Epoch 16 Training loss 2.810019255270775\n",
      "Epoch 17 iteration 0 loss 2.1519689559936523\n",
      "Epoch 17 iteration 100 loss 2.771408796310425\n",
      "Epoch 17 iteration 200 loss 2.3121986389160156\n",
      "Epoch 17 Training loss 2.7696067844956644\n",
      "Epoch 18 iteration 0 loss 2.087712287902832\n",
      "Epoch 18 iteration 100 loss 2.725250482559204\n",
      "Epoch 18 iteration 200 loss 2.3234703540802\n",
      "Epoch 18 Training loss 2.729185024002167\n",
      "Epoch 19 iteration 0 loss 2.0795533657073975\n",
      "Epoch 19 iteration 100 loss 2.7172603607177734\n",
      "Epoch 19 iteration 200 loss 2.2564520835876465\n",
      "Epoch 19 Training loss 2.688426630175405\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你有一些蘋果。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你是个人。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "他的房子裡了他的。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "那是什么？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我很喜歡這個。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "这是你的。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們在這裡。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "這個房間是很多。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它是很多。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "人都是他的房子。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "她的意思。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "你的是很好的。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我是他的。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜歡茶。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆不会让玛丽。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "請把門關門。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆是个苹果。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請把它來說話。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "警察逮捕了。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我在一起。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([en_id2word[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([cn_id2word[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_word2id[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [cn_id2word[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))\n",
    "\n",
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq包含Attention机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0  )\n",
    "\n",
    "        return out, hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, output, context, mask):\n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        # context: batch_size, context_len, 2*enc_hidden_size\n",
    "        # 此处output就相当于原文公式中的 ht; context相当于原文公式中的 hs拔\n",
    "        \n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        \n",
    "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(                \n",
    "            batch_size, input_len, -1) # batch_size, context_len, dec_hidden_size\n",
    "        \n",
    "        # context_in.transpose(1,2): batch_size, dec_hidden_size, context_len \n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        attn = torch.bmm(output, context_in.transpose(1,2)) \n",
    "        # batch_size, output_len, context_len\n",
    "\n",
    "        attn.data.masked_fill(mask, -1e6)\n",
    "\n",
    "        attn = F.softmax(attn, dim=2) \n",
    "        # batch_size, output_len, context_len\n",
    "\n",
    "        context = torch.bmm(attn, context) \n",
    "        # batch_size, output_len, enc_hidden_size\n",
    "        \n",
    "        output = torch.cat((context, output), dim=2) # batch_size, output_len, hidden_size*2\n",
    "\n",
    "        output = output.view(batch_size*output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):\n",
    "        # a mask of shape x_len * y_len\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "#         mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        mask = (~x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        \n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "\n",
    "        output, attn = self.attention(output_seq, ctx, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        \n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "embed_size = hidden_size = 100\n",
    "encoder = Encoder(vocab_size=en_total_words,\n",
    "                       embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words,\n",
    "                      embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.060481071472168\n",
      "Epoch 0 iteration 100 loss 5.35421895980835\n",
      "Epoch 0 iteration 200 loss 4.6912078857421875\n",
      "Epoch 0 Training loss 5.512168247363247\n",
      "Evaluation loss 5.008288016796525\n",
      "Epoch 1 iteration 0 loss 4.5456743240356445\n",
      "Epoch 1 iteration 100 loss 4.864917278289795\n",
      "Epoch 1 iteration 200 loss 4.151533603668213\n",
      "Epoch 1 Training loss 4.785248889403773\n",
      "Epoch 2 iteration 0 loss 3.9787633419036865\n",
      "Epoch 2 iteration 100 loss 4.431112766265869\n",
      "Epoch 2 iteration 200 loss 3.711575746536255\n",
      "Epoch 2 Training loss 4.3298231455745935\n",
      "Epoch 3 iteration 0 loss 3.5248868465423584\n",
      "Epoch 3 iteration 100 loss 4.094301700592041\n",
      "Epoch 3 iteration 200 loss 3.421085834503174\n",
      "Epoch 3 Training loss 3.9940971658770117\n",
      "Epoch 4 iteration 0 loss 3.166511058807373\n",
      "Epoch 4 iteration 100 loss 3.8226892948150635\n",
      "Epoch 4 iteration 200 loss 3.1802239418029785\n",
      "Epoch 4 Training loss 3.7325022364759595\n",
      "Epoch 5 iteration 0 loss 2.9333128929138184\n",
      "Epoch 5 iteration 100 loss 3.555757522583008\n",
      "Epoch 5 iteration 200 loss 2.9867947101593018\n",
      "Epoch 5 Training loss 3.513995611579871\n",
      "Evaluation loss 3.5410348500092264\n",
      "Epoch 6 iteration 0 loss 2.766754627227783\n",
      "Epoch 6 iteration 100 loss 3.399785041809082\n",
      "Epoch 6 iteration 200 loss 2.8240702152252197\n",
      "Epoch 6 Training loss 3.329983014075456\n",
      "Epoch 7 iteration 0 loss 2.5783891677856445\n",
      "Epoch 7 iteration 100 loss 3.1703035831451416\n",
      "Epoch 7 iteration 200 loss 2.692492961883545\n",
      "Epoch 7 Training loss 3.173619339180285\n",
      "Epoch 8 iteration 0 loss 2.452934503555298\n",
      "Epoch 8 iteration 100 loss 3.065587043762207\n",
      "Epoch 8 iteration 200 loss 2.542017698287964\n",
      "Epoch 8 Training loss 3.0322180344440275\n",
      "Epoch 9 iteration 0 loss 2.3368887901306152\n",
      "Epoch 9 iteration 100 loss 2.9091763496398926\n",
      "Epoch 9 iteration 200 loss 2.4133405685424805\n",
      "Epoch 9 Training loss 2.9123425029094454\n",
      "Epoch 10 iteration 0 loss 2.247685432434082\n",
      "Epoch 10 iteration 100 loss 2.786271333694458\n",
      "Epoch 10 iteration 200 loss 2.332963705062866\n",
      "Epoch 10 Training loss 2.801941798437588\n",
      "Evaluation loss 3.09767382130204\n",
      "Epoch 11 iteration 0 loss 2.0960686206817627\n",
      "Epoch 11 iteration 100 loss 2.6817588806152344\n",
      "Epoch 11 iteration 200 loss 2.235600233078003\n",
      "Epoch 11 Training loss 2.698876940891663\n",
      "Epoch 12 iteration 0 loss 2.039980411529541\n",
      "Epoch 12 iteration 100 loss 2.626685380935669\n",
      "Epoch 12 iteration 200 loss 2.1839115619659424\n",
      "Epoch 12 Training loss 2.6106073382664645\n",
      "Epoch 13 iteration 0 loss 1.910556435585022\n",
      "Epoch 13 iteration 100 loss 2.51643705368042\n",
      "Epoch 13 iteration 200 loss 2.092724561691284\n",
      "Epoch 13 Training loss 2.5266494557578594\n",
      "Epoch 14 iteration 0 loss 1.8665359020233154\n",
      "Epoch 14 iteration 100 loss 2.4548587799072266\n",
      "Epoch 14 iteration 200 loss 2.0181684494018555\n",
      "Epoch 14 Training loss 2.451447914749202\n",
      "Epoch 15 iteration 0 loss 1.7826647758483887\n",
      "Epoch 15 iteration 100 loss 2.3721721172332764\n",
      "Epoch 15 iteration 200 loss 1.931913137435913\n",
      "Epoch 15 Training loss 2.376598914042978\n",
      "Evaluation loss 2.9108677381932058\n",
      "Epoch 16 iteration 0 loss 1.7108263969421387\n",
      "Epoch 16 iteration 100 loss 2.320082664489746\n",
      "Epoch 16 iteration 200 loss 1.8738192319869995\n",
      "Epoch 16 Training loss 2.3157873033598118\n",
      "Epoch 17 iteration 0 loss 1.6979939937591553\n",
      "Epoch 17 iteration 100 loss 2.2206552028656006\n",
      "Epoch 17 iteration 200 loss 1.8212825059890747\n",
      "Epoch 17 Training loss 2.2566323452728017\n",
      "Epoch 18 iteration 0 loss 1.572745442390442\n",
      "Epoch 18 iteration 100 loss 2.1549124717712402\n",
      "Epoch 18 iteration 200 loss 1.8002402782440186\n",
      "Epoch 18 Training loss 2.199297001273995\n",
      "Epoch 19 iteration 0 loss 1.521057367324829\n",
      "Epoch 19 iteration 100 loss 2.1289732456207275\n",
      "Epoch 19 iteration 200 loss 1.722698450088501\n",
      "Epoch 19 Training loss 2.147129950568246\n",
      "Epoch 20 iteration 0 loss 1.5176575183868408\n",
      "Epoch 20 iteration 100 loss 2.1045327186584473\n",
      "Epoch 20 iteration 200 loss 1.747265100479126\n",
      "Epoch 20 Training loss 2.0941469294167803\n",
      "Evaluation loss 2.8205751667573695\n",
      "Epoch 21 iteration 0 loss 1.4370895624160767\n",
      "Epoch 21 iteration 100 loss 2.0293538570404053\n",
      "Epoch 21 iteration 200 loss 1.6701048612594604\n",
      "Epoch 21 Training loss 2.046778507184172\n",
      "Epoch 22 iteration 0 loss 1.3910869359970093\n",
      "Epoch 22 iteration 100 loss 1.9295947551727295\n",
      "Epoch 22 iteration 200 loss 1.5810719728469849\n",
      "Epoch 22 Training loss 2.001505016726451\n",
      "Epoch 23 iteration 0 loss 1.3303669691085815\n",
      "Epoch 23 iteration 100 loss 1.9293663501739502\n",
      "Epoch 23 iteration 200 loss 1.575695514678955\n",
      "Epoch 23 Training loss 1.958798502768376\n",
      "Epoch 24 iteration 0 loss 1.3381474018096924\n",
      "Epoch 24 iteration 100 loss 1.9170994758605957\n",
      "Epoch 24 iteration 200 loss 1.6155493259429932\n",
      "Epoch 24 Training loss 1.9196117879968542\n",
      "Epoch 25 iteration 0 loss 1.2895132303237915\n",
      "Epoch 25 iteration 100 loss 1.813485026359558\n",
      "Epoch 25 iteration 200 loss 1.541986346244812\n",
      "Epoch 25 Training loss 1.8838181033074914\n",
      "Evaluation loss 2.7924297438922183\n",
      "Epoch 26 iteration 0 loss 1.2691762447357178\n",
      "Epoch 26 iteration 100 loss 1.8375895023345947\n",
      "Epoch 26 iteration 200 loss 1.4797548055648804\n",
      "Epoch 26 Training loss 1.8466589936206903\n",
      "Epoch 27 iteration 0 loss 1.260345697402954\n",
      "Epoch 27 iteration 100 loss 1.7801960706710815\n",
      "Epoch 27 iteration 200 loss 1.4398096799850464\n",
      "Epoch 27 Training loss 1.8121397297318975\n",
      "Epoch 28 iteration 0 loss 1.235080361366272\n",
      "Epoch 28 iteration 100 loss 1.7467738389968872\n",
      "Epoch 28 iteration 200 loss 1.4107097387313843\n",
      "Epoch 28 Training loss 1.7764931838429572\n",
      "Epoch 29 iteration 0 loss 1.18618905544281\n",
      "Epoch 29 iteration 100 loss 1.6940721273422241\n",
      "Epoch 29 iteration 200 loss 1.3779529333114624\n",
      "Epoch 29 Training loss 1.745835104101929\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你有很多的。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你们是在做的。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "每个人都是他的。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "什么时候是什么？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我今晚有空。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "这本书的书。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們在午餐午餐了。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "这条椅子是安全。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "这是真的。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "许多上了他的房子。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "不会是它是不能的。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "有人们在看你。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我覺得他的臉。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜欢音樂。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆没有孩子。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "請關門。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆有点。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請說一些東西。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "保持续下一個星期。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我犯了一個錯誤。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 胡言乱语         \n",
    "- 对于本文所实现的两种机器翻译模型：\n",
    "- **训练模型过程中** ：对于Encoder部分，每一个cell的输入都取自上一个cell理论上的真实输出，而不是实际上的输出，具体可看**`class Encoder`**和**`class PlainEncoder()`**部分\n",
    "- **利用模型预测的过程**：对于Encoder部分，每一个cell的输入都取自上一个cell的实际输出，具体参考**`model.translate()`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
