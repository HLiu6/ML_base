{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "\n",
    "#忽略警告\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "### 加载数据\n",
    "- 将每行数据分成两部分：英语&中文\n",
    "- 在分割完成的英文和中文数据，开头加上\"BOS\",结尾加上\"EOS\"\n",
    "- 英文数据使用nltk进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    en=[]\n",
    "    cn=[]\n",
    "    num_examples=0\n",
    "    with open(in_file,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip().split(\"\\t\")\n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            # split chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "train_file = \"data/en-cn/train.txt\"\n",
    "dev_file = \"data/en-cn/dev.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建单词表\n",
    "`collections.Counter`用法\n",
    "- counter工具用于支持便捷和快速的计数，例如：\n",
    "``` python\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:\n",
    "    cnt[word] += 1\n",
    "print cnt\n",
    "输出:Counter({'blue': 3, 'red': 2, 'green': 1})\n",
    "```\n",
    "- `counter.most_common(n)`从多到少返回一个长度为n的列表\n",
    "``` python\n",
    "输入：Counter('abracadabra').most_common(3)\n",
    "输出：[('a', 5), ('r', 2), ('b', 2)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX=0\n",
    "PAD_IDX=1\n",
    "def build_dict(sentences,max_words=50000):\n",
    "    word_count=Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s]+=1\n",
    "    ls=word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 2\n",
    "    word2id = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    word2id[\"UNK\"] = UNK_IDX\n",
    "    word2id[\"PAD\"] = PAD_IDX\n",
    "    return word2id, total_words\n",
    "\n",
    "en_word2id, en_total_words = build_dict(train_en)\n",
    "cn_word2id, cn_total_words = build_dict(train_cn)\n",
    "en_id2word = {v: k for k, v in en_word2id.items()}\n",
    "cn_id2word = {v: k for k, v in cn_word2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将单词转变为数字\n",
    "- `dict.get(key,default=None)`\n",
    "    - key -- 字典中要查找的键\n",
    "    - default -- 如果指定键的值不存在时，返回该默认值\n",
    "    ``` python \n",
    "    dict = {'Name': 'Runoob', 'Age': 27}\n",
    "    print \"Value : %s\" %  dict.get('Age')\n",
    "    print \"Value : %s\" %  dict.get('Sex', \"Never\")\n",
    "    ```\n",
    "    - 输出：\n",
    "        * Value : 27\n",
    "        - Value : Never"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_word2id, cn_word2id, sort_by_len=True):\n",
    "\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_word2id.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_word2id.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_word2id, cn_word2id)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_word2id, cn_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS 我 有 很 多 才 能 。 EOS\n",
      "BOS i have many abilities . EOS\n"
     ]
    }
   ],
   "source": [
    "#查看数据集\n",
    "k = 1000\n",
    "print(\" \".join([cn_id2word[i] for i in train_cn[k]]))\n",
    "print(\" \".join([en_id2word[i] for i in train_en[k]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据集划分为batch\n",
    "- **`get_minibatches(n, batch_size, shuffle=True)`**\n",
    "    - n：数据集的大小\n",
    "    - batch_size：batch的大小\n",
    "    - shuffle：是否对原数据进行打乱\n",
    "    - 返回(batches)：列表(其中包含若干个长度为batch_size大小的数组)\n",
    "- **`prepare_data(seqs)`**\n",
    "    - seqs：一个batch的数据\n",
    "    - 该函数是将一个batch的数据转换成矩阵的形式\n",
    "    - 该矩阵的维度：高度为该batch中数据的个数；宽度为该batch中最长的数据的长度\n",
    "    - 返回：x(该矩阵)，x_lengths(该batch中每条数据的长度)\n",
    "- **`gen_examples(en_sentences, cn_sentences, batch_size)`**\n",
    "    - en_sentences:英文数据\n",
    "    - cn_sentences:中文数据\n",
    "    - batch_size:每个batch的大小\n",
    "    - 返回(all_ex):列表,其中包含若干个元组，每个元组有四个元素:\n",
    "        - 英文数据矩阵\n",
    "         ；英文数据长度\n",
    "        ；中文数据矩阵\n",
    "        ；中文数据长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(n, batch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, batch_size) # [0, 1, ..., n-1]\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    batches = []\n",
    "    for idx in idx_list:\n",
    "        batches.append(np.arange(idx, min(idx + batch_size, n)))\n",
    "    return batches\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths #x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq(无Attention机制)\n",
    "### 定义模型(无Attention机制)\n",
    "> ####  PlainEncoder()\n",
    ">     \n",
    "- **`torch.sort(input, dim=-1, descending=False, out=None) -> (Tensor, LongTensor)`**\n",
    "    - [Pytorch官方文档](https://pytorch.org/docs/stable/torch.html#torch.sort)\n",
    "    - 返回：排序后的数据；排序后的数据原本所处的位置\n",
    "- **`nn.utils.rnn.pack_padded_sequence & pad_packed_sequence`**\n",
    "    - [Pytorch官方文档](https://pytorch.org/docs/stable/nn.html#pack-padded-sequence)\n",
    "    - 主要是为了解决RNN的输入数据，不等长的问题\n",
    "- **`torch.Tensor.contiguous()`**\n",
    "    - [Pytorch官方文档](https://pytorch.org/docs/stable/tensors.html?highlight=contiguous#torch.Tensor.contiguous)\n",
    "    - 主要是整合数据，让数据在内存上连续存在之类的\n",
    "- 返回hid[[-1]]是为了取最后一层的最后一个cell的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        #这里要排序的原因，是因为对于pack_padded_sequence要求输入是排过序的\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        \n",
    "        #此处相当于对一个tensor调了下顺序，然后在物理内存上再利用contiguous()将数据放在一起\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        return out, hid[[-1]]\n",
    "\n",
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "#         print(output_seq.shape)\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        output = F.log_softmax(self.out(output_seq), -1)\n",
    "        \n",
    "        return output, hid\n",
    "    \n",
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid = self.decoder(y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, None\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=10):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            \n",
    "        return torch.cat(preds, 1), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        # input: (batch_size * seq_len) * vocab_size\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        # target: batch_size * 1\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encoder = PlainEncoder(vocab_size=en_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = PlainSeq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义验证、训练以及测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            # 更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "\n",
    "                \n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.10999870300293\n",
      "Epoch 0 iteration 100 loss 5.194458484649658\n",
      "Epoch 0 iteration 200 loss 5.378503322601318\n",
      "Epoch 0 Training loss 5.472302010920261\n",
      "Evaluation loss 4.846975916056102\n",
      "Epoch 1 iteration 0 loss 5.308679103851318\n",
      "Epoch 1 iteration 100 loss 4.5411224365234375\n",
      "Epoch 1 iteration 200 loss 4.858755111694336\n",
      "Epoch 1 Training loss 4.605002113540477\n",
      "Epoch 2 iteration 0 loss 4.845099925994873\n",
      "Epoch 2 iteration 100 loss 4.106603145599365\n",
      "Epoch 2 iteration 200 loss 4.532668590545654\n",
      "Epoch 2 Training loss 4.190401199153088\n",
      "Epoch 3 iteration 0 loss 4.543196201324463\n",
      "Epoch 3 iteration 100 loss 3.8407373428344727\n",
      "Epoch 3 iteration 200 loss 4.312853813171387\n",
      "Epoch 3 Training loss 3.9261740795265876\n",
      "Epoch 4 iteration 0 loss 4.331034183502197\n",
      "Epoch 4 iteration 100 loss 3.641451358795166\n",
      "Epoch 4 iteration 200 loss 4.148542881011963\n",
      "Epoch 4 Training loss 3.731367938867295\n",
      "Epoch 5 iteration 0 loss 4.1790971755981445\n",
      "Epoch 5 iteration 100 loss 3.4977612495422363\n",
      "Epoch 5 iteration 200 loss 4.025062084197998\n",
      "Epoch 5 Training loss 3.5769636800559987\n",
      "Evaluation loss 3.6450447066359684\n",
      "Epoch 6 iteration 0 loss 4.032096862792969\n",
      "Epoch 6 iteration 100 loss 3.4033119678497314\n",
      "Epoch 6 iteration 200 loss 3.92081880569458\n",
      "Epoch 6 Training loss 3.4477665930130503\n",
      "Epoch 7 iteration 0 loss 3.920480728149414\n",
      "Epoch 7 iteration 100 loss 3.2907962799072266\n",
      "Epoch 7 iteration 200 loss 3.8478896617889404\n",
      "Epoch 7 Training loss 3.3394032752110423\n",
      "Epoch 8 iteration 0 loss 3.8328113555908203\n",
      "Epoch 8 iteration 100 loss 3.203317642211914\n",
      "Epoch 8 iteration 200 loss 3.7560908794403076\n",
      "Epoch 8 Training loss 3.2457087934174433\n",
      "Epoch 9 iteration 0 loss 3.7340283393859863\n",
      "Epoch 9 iteration 100 loss 3.1139566898345947\n",
      "Epoch 9 iteration 200 loss 3.6690104007720947\n",
      "Epoch 9 Training loss 3.162660799964552\n",
      "Epoch 10 iteration 0 loss 3.6684927940368652\n",
      "Epoch 10 iteration 100 loss 3.058931827545166\n",
      "Epoch 10 iteration 200 loss 3.6623408794403076\n",
      "Epoch 10 Training loss 3.0902409661867236\n",
      "Evaluation loss 3.3731125602129315\n",
      "Epoch 11 iteration 0 loss 3.599430799484253\n",
      "Epoch 11 iteration 100 loss 2.983259677886963\n",
      "Epoch 11 iteration 200 loss 3.601969003677368\n",
      "Epoch 11 Training loss 3.0197696893199426\n",
      "Epoch 12 iteration 0 loss 3.5170609951019287\n",
      "Epoch 12 iteration 100 loss 2.9507360458374023\n",
      "Epoch 12 iteration 200 loss 3.5724406242370605\n",
      "Epoch 12 Training loss 2.959806485756752\n",
      "Epoch 13 iteration 0 loss 3.5303080081939697\n",
      "Epoch 13 iteration 100 loss 2.8906681537628174\n",
      "Epoch 13 iteration 200 loss 3.5211806297302246\n",
      "Epoch 13 Training loss 2.90257034553751\n",
      "Epoch 14 iteration 0 loss 3.4338417053222656\n",
      "Epoch 14 iteration 100 loss 2.8369812965393066\n",
      "Epoch 14 iteration 200 loss 3.4624154567718506\n",
      "Epoch 14 Training loss 2.848855162913507\n",
      "Epoch 15 iteration 0 loss 3.4139602184295654\n",
      "Epoch 15 iteration 100 loss 2.798367738723755\n",
      "Epoch 15 iteration 200 loss 3.4208459854125977\n",
      "Epoch 15 Training loss 2.7965454293356498\n",
      "Evaluation loss 3.2545236377884477\n",
      "Epoch 16 iteration 0 loss 3.362260103225708\n",
      "Epoch 16 iteration 100 loss 2.7273311614990234\n",
      "Epoch 16 iteration 200 loss 3.3604238033294678\n",
      "Epoch 16 Training loss 2.750749940761247\n",
      "Epoch 17 iteration 0 loss 3.3418891429901123\n",
      "Epoch 17 iteration 100 loss 2.704688549041748\n",
      "Epoch 17 iteration 200 loss 3.327166795730591\n",
      "Epoch 17 Training loss 2.7092673860222805\n",
      "Epoch 18 iteration 0 loss 3.2673087120056152\n",
      "Epoch 18 iteration 100 loss 2.681291341781616\n",
      "Epoch 18 iteration 200 loss 3.309345006942749\n",
      "Epoch 18 Training loss 2.6688486853876396\n",
      "Epoch 19 iteration 0 loss 3.2515594959259033\n",
      "Epoch 19 iteration 100 loss 2.645458936691284\n",
      "Epoch 19 iteration 200 loss 3.2873666286468506\n",
      "Epoch 19 Training loss 2.6293539569587208\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你有一個好人。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你在做什麼事。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "他的父親是他的父親。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "有什么时候？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我今天下午。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "你的車子有一本書。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們在這裡。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "这是个苹果。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它是个好。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "他的父親是他的。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "别再试试。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "有人的你都是个好人。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我在他的房子。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜歡爵士樂。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆不知道。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "請把門關上。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆是个人。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請你一個好人。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "今天有一个苹果。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我的意思是一个。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([en_id2word[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([cn_id2word[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_word2id[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [cn_id2word[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))\n",
    "\n",
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq包含Attention机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0  )\n",
    "\n",
    "        return out, hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, output, context, mask):\n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        # context: batch_size, context_len, 2*enc_hidden_size\n",
    "        # 此处output就相当于原文公式中的 ht; context相当于原文公式中的 hs拔\n",
    "        \n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        \n",
    "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(                \n",
    "            batch_size, input_len, -1) # batch_size, context_len, dec_hidden_size\n",
    "        \n",
    "        # context_in.transpose(1,2): batch_size, dec_hidden_size, context_len \n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        attn = torch.bmm(output, context_in.transpose(1,2)) \n",
    "        # batch_size, output_len, context_len\n",
    "\n",
    "        attn.data.masked_fill(mask, -1e6)\n",
    "\n",
    "        attn = F.softmax(attn, dim=2) \n",
    "        # batch_size, output_len, context_len\n",
    "\n",
    "        context = torch.bmm(attn, context) \n",
    "        # batch_size, output_len, enc_hidden_size\n",
    "        \n",
    "        output = torch.cat((context, output), dim=2) # batch_size, output_len, hidden_size*2\n",
    "\n",
    "        output = output.view(batch_size*output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):\n",
    "        # a mask of shape x_len * y_len\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "#         mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        mask = (~x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        \n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "\n",
    "        output, attn = self.attention(output_seq, ctx, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        \n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "embed_size = hidden_size = 100\n",
    "encoder = Encoder(vocab_size=en_total_words,\n",
    "                       embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words,\n",
    "                      embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.074338912963867\n",
      "Epoch 0 iteration 100 loss 5.239917755126953\n",
      "Epoch 0 iteration 200 loss 5.501195430755615\n",
      "Epoch 0 Training loss 5.4583675025075475\n",
      "Evaluation loss 4.990075687667783\n",
      "Epoch 1 iteration 0 loss 5.441702842712402\n",
      "Epoch 1 iteration 100 loss 4.830410003662109\n",
      "Epoch 1 iteration 200 loss 5.0964250564575195\n",
      "Epoch 1 Training loss 4.8115485397896975\n",
      "Epoch 2 iteration 0 loss 5.061765670776367\n",
      "Epoch 2 iteration 100 loss 4.367208957672119\n",
      "Epoch 2 iteration 200 loss 4.726598262786865\n",
      "Epoch 2 Training loss 4.39293097572089\n",
      "Epoch 3 iteration 0 loss 4.7129034996032715\n",
      "Epoch 3 iteration 100 loss 4.004227161407471\n",
      "Epoch 3 iteration 200 loss 4.442802906036377\n",
      "Epoch 3 Training loss 4.052202010597128\n",
      "Epoch 4 iteration 0 loss 4.454996585845947\n",
      "Epoch 4 iteration 100 loss 3.7315001487731934\n",
      "Epoch 4 iteration 200 loss 4.232832431793213\n",
      "Epoch 4 Training loss 3.7945316203644133\n",
      "Epoch 5 iteration 0 loss 4.212111949920654\n",
      "Epoch 5 iteration 100 loss 3.5149412155151367\n",
      "Epoch 5 iteration 200 loss 4.074446678161621\n",
      "Epoch 5 Training loss 3.5910541437724093\n",
      "Evaluation loss 3.603808311166852\n",
      "Epoch 6 iteration 0 loss 4.031292915344238\n",
      "Epoch 6 iteration 100 loss 3.3361456394195557\n",
      "Epoch 6 iteration 200 loss 3.909707546234131\n",
      "Epoch 6 Training loss 3.417868363868951\n",
      "Epoch 7 iteration 0 loss 3.903001546859741\n",
      "Epoch 7 iteration 100 loss 3.2040371894836426\n",
      "Epoch 7 iteration 200 loss 3.7708332538604736\n",
      "Epoch 7 Training loss 3.2714780698472787\n",
      "Epoch 8 iteration 0 loss 3.7897377014160156\n",
      "Epoch 8 iteration 100 loss 3.094815969467163\n",
      "Epoch 8 iteration 200 loss 3.7099766731262207\n",
      "Epoch 8 Training loss 3.1411388818140997\n",
      "Epoch 9 iteration 0 loss 3.684183120727539\n",
      "Epoch 9 iteration 100 loss 2.94600248336792\n",
      "Epoch 9 iteration 200 loss 3.6107804775238037\n",
      "Epoch 9 Training loss 3.029029412549232\n",
      "Epoch 10 iteration 0 loss 3.5713939666748047\n",
      "Epoch 10 iteration 100 loss 2.864819049835205\n",
      "Epoch 10 iteration 200 loss 3.524765968322754\n",
      "Epoch 10 Training loss 2.9278040362223927\n",
      "Evaluation loss 3.2102675669129868\n",
      "Epoch 11 iteration 0 loss 3.503086566925049\n",
      "Epoch 11 iteration 100 loss 2.782541036605835\n",
      "Epoch 11 iteration 200 loss 3.447904586791992\n",
      "Epoch 11 Training loss 2.8345692116560213\n",
      "Epoch 12 iteration 0 loss 3.4410922527313232\n",
      "Epoch 12 iteration 100 loss 2.704779863357544\n",
      "Epoch 12 iteration 200 loss 3.3779664039611816\n",
      "Epoch 12 Training loss 2.7502750824206394\n",
      "Epoch 13 iteration 0 loss 3.369866371154785\n",
      "Epoch 13 iteration 100 loss 2.6296777725219727\n",
      "Epoch 13 iteration 200 loss 3.319701671600342\n",
      "Epoch 13 Training loss 2.674227444067466\n",
      "Epoch 14 iteration 0 loss 3.2996826171875\n",
      "Epoch 14 iteration 100 loss 2.557647943496704\n",
      "Epoch 14 iteration 200 loss 3.24536395072937\n",
      "Epoch 14 Training loss 2.603622213720769\n",
      "Epoch 15 iteration 0 loss 3.2573459148406982\n",
      "Epoch 15 iteration 100 loss 2.467726945877075\n",
      "Epoch 15 iteration 200 loss 3.1590514183044434\n",
      "Epoch 15 Training loss 2.53583803985891\n",
      "Evaluation loss 3.0390762792139574\n",
      "Epoch 16 iteration 0 loss 3.204432249069214\n",
      "Epoch 16 iteration 100 loss 2.435227394104004\n",
      "Epoch 16 iteration 200 loss 3.113847017288208\n",
      "Epoch 16 Training loss 2.4757410854751907\n",
      "Epoch 17 iteration 0 loss 3.1242733001708984\n",
      "Epoch 17 iteration 100 loss 2.39547061920166\n",
      "Epoch 17 iteration 200 loss 3.0870144367218018\n",
      "Epoch 17 Training loss 2.417634115262937\n",
      "Epoch 18 iteration 0 loss 3.1203482151031494\n",
      "Epoch 18 iteration 100 loss 2.3281521797180176\n",
      "Epoch 18 iteration 200 loss 3.056631565093994\n",
      "Epoch 18 Training loss 2.359491924189237\n",
      "Epoch 19 iteration 0 loss 3.0298547744750977\n",
      "Epoch 19 iteration 100 loss 2.2939133644104004\n",
      "Epoch 19 iteration 200 loss 2.992341995239258\n",
      "Epoch 19 Training loss 2.3134212581688245\n",
      "Epoch 20 iteration 0 loss 2.9752602577209473\n",
      "Epoch 20 iteration 100 loss 2.235849618911743\n",
      "Epoch 20 iteration 200 loss 2.949920415878296\n",
      "Epoch 20 Training loss 2.260017586155313\n",
      "Evaluation loss 2.947547029634837\n",
      "Epoch 21 iteration 0 loss 2.936697006225586\n",
      "Epoch 21 iteration 100 loss 2.193298578262329\n",
      "Epoch 21 iteration 200 loss 2.922743082046509\n",
      "Epoch 21 Training loss 2.213435187586504\n",
      "Epoch 22 iteration 0 loss 2.916138172149658\n",
      "Epoch 22 iteration 100 loss 2.1082351207733154\n",
      "Epoch 22 iteration 200 loss 2.8363139629364014\n",
      "Epoch 22 Training loss 2.1660468261654784\n",
      "Epoch 23 iteration 0 loss 2.821120500564575\n",
      "Epoch 23 iteration 100 loss 2.090522289276123\n",
      "Epoch 23 iteration 200 loss 2.8215036392211914\n",
      "Epoch 23 Training loss 2.1262493363230752\n",
      "Epoch 24 iteration 0 loss 2.833211660385132\n",
      "Epoch 24 iteration 100 loss 2.0199286937713623\n",
      "Epoch 24 iteration 200 loss 2.810394763946533\n",
      "Epoch 24 Training loss 2.084639900606623\n",
      "Epoch 25 iteration 0 loss 2.8128812313079834\n",
      "Epoch 25 iteration 100 loss 1.97171950340271\n",
      "Epoch 25 iteration 200 loss 2.7737936973571777\n",
      "Epoch 25 Training loss 2.0463486759255987\n",
      "Evaluation loss 2.905496192600268\n",
      "Epoch 26 iteration 0 loss 2.7451021671295166\n",
      "Epoch 26 iteration 100 loss 1.9734764099121094\n",
      "Epoch 26 iteration 200 loss 2.713192939758301\n",
      "Epoch 26 Training loss 2.00951466885367\n",
      "Epoch 27 iteration 0 loss 2.7199769020080566\n",
      "Epoch 27 iteration 100 loss 1.9098232984542847\n",
      "Epoch 27 iteration 200 loss 2.671081781387329\n",
      "Epoch 27 Training loss 1.977190901217483\n",
      "Epoch 28 iteration 0 loss 2.6767938137054443\n",
      "Epoch 28 iteration 100 loss 1.933789849281311\n",
      "Epoch 28 iteration 200 loss 2.69539213180542\n",
      "Epoch 28 Training loss 1.9431724899267264\n",
      "Epoch 29 iteration 0 loss 2.6434569358825684\n",
      "Epoch 29 iteration 100 loss 1.8551899194717407\n",
      "Epoch 29 iteration 200 loss 2.6457669734954834\n",
      "Epoch 29 Training loss 1.910024728175129\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你有很多。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你在看看的。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "每个人都是他的妻子。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "它是什么时候？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我今晚玩得很晚。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "你这本书是你的。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們在吃午饭。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "这个苹果很大。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它是一切的。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "許多意外上有人叫他。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "停止。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "只是你的。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我和他的姐姐。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜歡爵士樂。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆没有孩子。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "請關門。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆下了个眼。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請在加拿大利的聲音。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "在星期下。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我做了一件錯的。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 胡言乱语         \n",
    "- 对于本文所实现的两种机器翻译模型：\n",
    "- **训练模型过程中** ：对于Encoder部分，每一个cell的输入都取自上一个cell理论上的真实输出，而不是实际上的输出，具体可看**`class Encoder`**和**`class PlainEncoder()`**部分\n",
    "- **利用模型预测的过程**：对于Encoder部分，每一个cell的输入都取自上一个cell的实际输出，具体参考**`model.translate()`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
